{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d678d188-129e-40c2-a9b2-d7411b355f35",
   "metadata": {},
   "source": [
    "## Data Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f03380-5bab-4d72-a8a5-0e8e0180f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"]=(20,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c3b0586-9263-4ad7-978f-708d200066be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>acc_xs_mean</th>\n",
       "      <th>acc_ys_mean</th>\n",
       "      <th>acc_zs_mean</th>\n",
       "      <th>acc_xs_var</th>\n",
       "      <th>acc_ys_var</th>\n",
       "      <th>acc_zs_var</th>\n",
       "      <th>acc_xs_mad</th>\n",
       "      <th>...</th>\n",
       "      <th>gps_bearing_min</th>\n",
       "      <th>gps_accuracy_min</th>\n",
       "      <th>gps_lat_iqr</th>\n",
       "      <th>gps_long_iqr</th>\n",
       "      <th>gps_alt_iqr</th>\n",
       "      <th>gps_speed_iqr</th>\n",
       "      <th>gps_bearing_iqr</th>\n",
       "      <th>gps_accuracy_iqr</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5372713</td>\n",
       "      <td>11</td>\n",
       "      <td>1.570541e+09</td>\n",
       "      <td>0.041235</td>\n",
       "      <td>0.074212</td>\n",
       "      <td>-0.001340</td>\n",
       "      <td>1.350480</td>\n",
       "      <td>1.550560</td>\n",
       "      <td>4.986602</td>\n",
       "      <td>0.590246</td>\n",
       "      <td>...</td>\n",
       "      <td>223.478317</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>3.877194</td>\n",
       "      <td>0.181761</td>\n",
       "      <td>13.691864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-497</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5372818</td>\n",
       "      <td>11</td>\n",
       "      <td>1.570541e+09</td>\n",
       "      <td>0.067678</td>\n",
       "      <td>0.075321</td>\n",
       "      <td>-0.005949</td>\n",
       "      <td>1.320684</td>\n",
       "      <td>1.509417</td>\n",
       "      <td>5.022726</td>\n",
       "      <td>0.590798</td>\n",
       "      <td>...</td>\n",
       "      <td>223.478317</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>3.877194</td>\n",
       "      <td>0.181761</td>\n",
       "      <td>13.691864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-497</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5372924</td>\n",
       "      <td>11</td>\n",
       "      <td>1.570541e+09</td>\n",
       "      <td>0.074119</td>\n",
       "      <td>0.107075</td>\n",
       "      <td>-0.020080</td>\n",
       "      <td>1.252855</td>\n",
       "      <td>1.481293</td>\n",
       "      <td>4.945844</td>\n",
       "      <td>0.575731</td>\n",
       "      <td>...</td>\n",
       "      <td>223.478317</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>3.877194</td>\n",
       "      <td>0.181761</td>\n",
       "      <td>13.691864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-497</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5373030</td>\n",
       "      <td>11</td>\n",
       "      <td>1.570541e+09</td>\n",
       "      <td>0.081470</td>\n",
       "      <td>0.088129</td>\n",
       "      <td>-0.017702</td>\n",
       "      <td>1.193755</td>\n",
       "      <td>1.453578</td>\n",
       "      <td>5.022796</td>\n",
       "      <td>0.573366</td>\n",
       "      <td>...</td>\n",
       "      <td>223.478317</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>1.938597</td>\n",
       "      <td>0.174963</td>\n",
       "      <td>6.943611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-497</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5373135</td>\n",
       "      <td>11</td>\n",
       "      <td>1.570541e+09</td>\n",
       "      <td>0.081440</td>\n",
       "      <td>0.082775</td>\n",
       "      <td>-0.041844</td>\n",
       "      <td>1.235434</td>\n",
       "      <td>1.469258</td>\n",
       "      <td>5.054491</td>\n",
       "      <td>0.592963</td>\n",
       "      <td>...</td>\n",
       "      <td>223.478317</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>1.057135</td>\n",
       "      <td>0.168166</td>\n",
       "      <td>0.195358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-497</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  user     timestamp  acc_xs_mean  acc_ys_mean  acc_zs_mean  \\\n",
       "0  5372713    11  1.570541e+09     0.041235     0.074212    -0.001340   \n",
       "1  5372818    11  1.570541e+09     0.067678     0.075321    -0.005949   \n",
       "2  5372924    11  1.570541e+09     0.074119     0.107075    -0.020080   \n",
       "3  5373030    11  1.570541e+09     0.081470     0.088129    -0.017702   \n",
       "4  5373135    11  1.570541e+09     0.081440     0.082775    -0.041844   \n",
       "\n",
       "   acc_xs_var  acc_ys_var  acc_zs_var  acc_xs_mad  ...  gps_bearing_min  \\\n",
       "0    1.350480    1.550560    4.986602    0.590246  ...       223.478317   \n",
       "1    1.320684    1.509417    5.022726    0.590798  ...       223.478317   \n",
       "2    1.252855    1.481293    4.945844    0.575731  ...       223.478317   \n",
       "3    1.193755    1.453578    5.022796    0.573366  ...       223.478317   \n",
       "4    1.235434    1.469258    5.054491    0.592963  ...       223.478317   \n",
       "\n",
       "   gps_accuracy_min  gps_lat_iqr  gps_long_iqr  gps_alt_iqr  gps_speed_iqr  \\\n",
       "0               6.0     0.000169      0.000128     3.877194       0.181761   \n",
       "1               6.0     0.000169      0.000128     3.877194       0.181761   \n",
       "2               6.0     0.000169      0.000128     3.877194       0.181761   \n",
       "3               6.0     0.000096      0.000080     1.938597       0.174963   \n",
       "4               6.0     0.000024      0.000032     1.057135       0.168166   \n",
       "\n",
       "   gps_bearing_iqr  gps_accuracy_iqr  activity_id  activity  \n",
       "0        13.691864               0.0         -497   Walking  \n",
       "1        13.691864               0.0         -497   Walking  \n",
       "2        13.691864               0.0         -497   Walking  \n",
       "3         6.943611               0.0         -497   Walking  \n",
       "4         0.195358               0.0         -497   Walking  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"sensoringData_feature_prepared_20_19.0_0_split_1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6875095f-da22-4c13-bba8-ee890080a450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60733, 59)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a026a86-bd08-4dfe-82b9-ed9a3f902f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>acc_xs_mean</th>\n",
       "      <th>acc_ys_mean</th>\n",
       "      <th>acc_zs_mean</th>\n",
       "      <th>acc_xs_var</th>\n",
       "      <th>acc_ys_var</th>\n",
       "      <th>acc_zs_var</th>\n",
       "      <th>acc_xs_mad</th>\n",
       "      <th>...</th>\n",
       "      <th>gps_bearing_min</th>\n",
       "      <th>gps_accuracy_min</th>\n",
       "      <th>gps_lat_iqr</th>\n",
       "      <th>gps_long_iqr</th>\n",
       "      <th>gps_alt_iqr</th>\n",
       "      <th>gps_speed_iqr</th>\n",
       "      <th>gps_bearing_iqr</th>\n",
       "      <th>gps_accuracy_iqr</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, user, timestamp, acc_xs_mean, acc_ys_mean, acc_zs_mean, acc_xs_var, acc_ys_var, acc_zs_var, acc_xs_mad, acc_ys_mad, acc_zs_mad, acc_xs_max, acc_ys_max, acc_zs_max, acc_xs_min, acc_ys_min, acc_zs_min, acc_xs_iqr, acc_ys_iqr, acc_zs_iqr, gps_lat_mean, gps_long_mean, gps_alt_mean, gps_speed_mean, gps_bearing_mean, gps_accuracy_mean, gps_lat_var, gps_long_var, gps_alt_var, gps_speed_var, gps_bearing_var, gps_accuracy_var, gps_lat_mad, gps_long_mad, gps_alt_mad, gps_speed_mad, gps_bearing_mad, gps_accuracy_mad, gps_lat_max, gps_long_max, gps_alt_max, gps_speed_max, gps_bearing_max, gps_accuracy_max, gps_lat_min, gps_long_min, gps_alt_min, gps_speed_min, gps_bearing_min, gps_accuracy_min, gps_lat_iqr, gps_long_iqr, gps_alt_iqr, gps_speed_iqr, gps_bearing_iqr, gps_accuracy_iqr, activity_id, activity]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 59 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.gps_lat_max>0.2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac37d823-999f-4046-851f-662177ce1978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'user', 'timestamp', 'acc_xs_mean', 'acc_ys_mean', 'acc_zs_mean',\n",
       "       'acc_xs_var', 'acc_ys_var', 'acc_zs_var', 'acc_xs_mad', 'acc_ys_mad',\n",
       "       'acc_zs_mad', 'acc_xs_max', 'acc_ys_max', 'acc_zs_max', 'acc_xs_min',\n",
       "       'acc_ys_min', 'acc_zs_min', 'acc_xs_iqr', 'acc_ys_iqr', 'acc_zs_iqr',\n",
       "       'gps_lat_mean', 'gps_long_mean', 'gps_alt_mean', 'gps_speed_mean',\n",
       "       'gps_bearing_mean', 'gps_accuracy_mean', 'gps_lat_var', 'gps_long_var',\n",
       "       'gps_alt_var', 'gps_speed_var', 'gps_bearing_var', 'gps_accuracy_var',\n",
       "       'gps_lat_mad', 'gps_long_mad', 'gps_alt_mad', 'gps_speed_mad',\n",
       "       'gps_bearing_mad', 'gps_accuracy_mad', 'gps_lat_max', 'gps_long_max',\n",
       "       'gps_alt_max', 'gps_speed_max', 'gps_bearing_max', 'gps_accuracy_max',\n",
       "       'gps_lat_min', 'gps_long_min', 'gps_alt_min', 'gps_speed_min',\n",
       "       'gps_bearing_min', 'gps_accuracy_min', 'gps_lat_iqr', 'gps_long_iqr',\n",
       "       'gps_alt_iqr', 'gps_speed_iqr', 'gps_bearing_iqr', 'gps_accuracy_iqr',\n",
       "       'activity_id', 'activity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9339179-2867-4778-b43a-57c3d3015dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_threshold = 0.2\n",
    "long_threshold = 0.2\n",
    "alt_threshold = 500\n",
    "\n",
    "# Filter out rows where GPS data exceeds the thresholds\n",
    "df1 = df[\n",
    "    (df['gps_lat_mean'].abs() <= lat_threshold) &\n",
    "    (df['gps_long_mean'].abs() <= long_threshold) &\n",
    "    (df['gps_alt_mean'].abs() <= alt_threshold)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d61a1842-fcb4-4bdb-bbe4-6a7e767c655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (60733, 54)\n",
      "Accuracy: 0.9697044537745946\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Active       0.95      0.99      0.97      4419\n",
      "     Driving       0.99      0.98      0.99      4327\n",
      "    Inactive       0.93      0.97      0.95      1046\n",
      "     Walking       0.98      0.92      0.95      2355\n",
      "\n",
      "    accuracy                           0.97     12147\n",
      "   macro avg       0.96      0.96      0.96     12147\n",
      "weighted avg       0.97      0.97      0.97     12147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming df1 is already loaded and cleaned\n",
    "\n",
    "# Step 1: Prepare the feature matrix (X) and target vector (y)\n",
    "# Only drop columns that are not relevant for training (excluding 'activity' and 'timestamp')\n",
    "X = df1.drop(columns=['id', 'activity_id', 'user', 'timestamp', 'activity'])  # Features\n",
    "y = df1['activity']  # Target variable ('walking', 'inactive', 'active', 'driving')\n",
    "\n",
    "# Check if X has valid data\n",
    "print(\"Shape of X:\", X.shape)  # Check the shape of the feature matrix\n",
    "\n",
    "# Handle missing data in X\n",
    "X = X.dropna()  # Dropping rows with missing values in features\n",
    "\n",
    "# If X is still empty, that means no data passed the dropna step, which means something went wrong\n",
    "if X.shape[0] == 0:\n",
    "    raise ValueError(\"No valid rows left in the feature set after handling missing data.\")\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the features (important for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train the SVM model with an RBF kernel\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f85a7f5-d358-41a6-9b0c-710836c131fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (60733, 54)\n",
      "Accuracy: 0.9337284926319256\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Active       0.90      0.96      0.93      4419\n",
      "     Driving       0.96      0.97      0.97      4327\n",
      "    Inactive       1.00      0.86      0.93      1046\n",
      "     Walking       0.93      0.85      0.89      2355\n",
      "\n",
      "    accuracy                           0.93     12147\n",
      "   macro avg       0.95      0.91      0.93     12147\n",
      "weighted avg       0.94      0.93      0.93     12147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Step 1: Prepare the feature matrix (X) and target vector (y)\n",
    "X = df1.drop(columns=['id', 'activity_id', 'user', 'timestamp', 'activity'])  # Features\n",
    "y = df1['activity']  # Target variable ('walking', 'inactive', 'active', 'driving')\n",
    "\n",
    "# Check if X has valid data\n",
    "print(\"Shape of X:\", X.shape)  # Check the shape of the feature matrix\n",
    "\n",
    "# Handle missing data in X\n",
    "X = X.dropna()  # Dropping rows with missing values in features\n",
    "\n",
    "# If X is still empty, that means no data passed the dropna step, which means something went wrong\n",
    "if X.shape[0] == 0:\n",
    "    raise ValueError(\"No valid rows left in the feature set after handling missing data.\")\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the features (important for models that are sensitive to scale like SVM, but optional for Decision Trees)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train the Decision Tree model with a maximum depth of 5\n",
    "dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = dt_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9c091be-4425-42bc-8e5f-9f17f40f8fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (60733, 54)\n",
      "Accuracy: 0.9984358277764057\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Active       1.00      1.00      1.00      4419\n",
      "     Driving       1.00      1.00      1.00      4327\n",
      "    Inactive       1.00      1.00      1.00      1046\n",
      "     Walking       0.99      1.00      1.00      2355\n",
      "\n",
      "    accuracy                           1.00     12147\n",
      "   macro avg       1.00      1.00      1.00     12147\n",
      "weighted avg       1.00      1.00      1.00     12147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming df1 is already loaded and cleaned\n",
    "\n",
    "# Step 1: Prepare the feature matrix (X) and target vector (y)\n",
    "X = df1.drop(columns=['id', 'activity_id', 'user', 'timestamp', 'activity'])  # Features\n",
    "y = df1['activity']  # Target variable ('walking', 'inactive', 'active', 'driving')\n",
    "\n",
    "# Check if X has valid data\n",
    "print(\"Shape of X:\", X.shape)  # Check the shape of the feature matrix\n",
    "\n",
    "# Handle missing data in X\n",
    "X = X.dropna()  # Dropping rows with missing values in features\n",
    "\n",
    "# If X is still empty, that means no data passed the dropna step, which means something went wrong\n",
    "if X.shape[0] == 0:\n",
    "    raise ValueError(\"No valid rows left in the feature set after handling missing data.\")\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train the MLP model with 100 hidden units\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = mlp_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c3063bf-1c63-4c32-b2a9-e1b48b27f1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (60733, 54)\n",
      "Accuracy: 0.7587881781509838\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Active       0.86      0.66      0.74      4419\n",
      "     Driving       0.97      0.84      0.90      4327\n",
      "    Inactive       0.31      0.93      0.46      1046\n",
      "     Walking       0.91      0.72      0.81      2355\n",
      "\n",
      "    accuracy                           0.76     12147\n",
      "   macro avg       0.76      0.79      0.73     12147\n",
      "weighted avg       0.86      0.76      0.79     12147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming df1 is already loaded and cleaned\n",
    "\n",
    "# Step 1: Prepare the feature matrix (X) and target vector (y)\n",
    "X = df1.drop(columns=['id', 'activity_id', 'user', 'timestamp', 'activity'])  # Features\n",
    "y = df1['activity']  # Target variable ('walking', 'inactive', 'active', 'driving')\n",
    "\n",
    "# Check if X has valid data\n",
    "print(\"Shape of X:\", X.shape)  # Check the shape of the feature matrix\n",
    "\n",
    "# Handle missing data in X\n",
    "X = X.dropna()  # Dropping rows with missing values in features\n",
    "\n",
    "# If X is still empty, that means no data passed the dropna step, which means something went wrong\n",
    "if X.shape[0] == 0:\n",
    "    raise ValueError(\"No valid rows left in the feature set after handling missing data.\")\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the features (Naive Bayes can be sensitive to feature scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train the Naive Bayes model (GaussianNB)\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = nb_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c4152cc-b9ad-4863-bf7d-5b90718ff898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (60733, 54)\n",
      "Accuracy: 0.9781015888696798\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Active       0.97      0.99      0.98      4419\n",
      "     Driving       0.98      0.98      0.98      4327\n",
      "    Inactive       0.99      0.97      0.98      1046\n",
      "     Walking       0.97      0.95      0.96      2355\n",
      "\n",
      "    accuracy                           0.98     12147\n",
      "   macro avg       0.98      0.97      0.98     12147\n",
      "weighted avg       0.98      0.98      0.98     12147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming df1 is already loaded and cleaned\n",
    "\n",
    "# Step 1: Prepare the feature matrix (X) and target vector (y)\n",
    "X = df1.drop(columns=['id', 'activity_id', 'user', 'timestamp', 'activity'])  # Features\n",
    "y = df1['activity']  # Target variable ('walking', 'inactive', 'active', 'driving')\n",
    "\n",
    "# Check if X has valid data\n",
    "print(\"Shape of X:\", X.shape)  # Check the shape of the feature matrix\n",
    "\n",
    "# Handle missing data in X\n",
    "X = X.dropna()  # Dropping rows with missing values in features\n",
    "\n",
    "# If X is still empty, that means no data passed the dropna step, which means something went wrong\n",
    "if X.shape[0] == 0:\n",
    "    raise ValueError(\"No valid rows left in the feature set after handling missing data.\")\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the features (KNN is sensitive to feature scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train the KNN model with 34 neighbors\n",
    "knn_model = KNeighborsClassifier(n_neighbors=34)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d65e175-d40c-4e7a-87e0-e90e1d621313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (60733, 54)\n",
      "Accuracy: 0.9997530254383798\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Active       1.00      1.00      1.00      4419\n",
      "     Driving       1.00      1.00      1.00      4327\n",
      "    Inactive       1.00      1.00      1.00      1046\n",
      "     Walking       1.00      1.00      1.00      2355\n",
      "\n",
      "    accuracy                           1.00     12147\n",
      "   macro avg       1.00      1.00      1.00     12147\n",
      "weighted avg       1.00      1.00      1.00     12147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming df1 is already loaded and cleaned\n",
    "\n",
    "# Step 1: Prepare the feature matrix (X) and target vector (y)\n",
    "X = df1.drop(columns=['id', 'activity_id', 'user', 'timestamp', 'activity'])  # Features\n",
    "y = df1['activity']  # Target variable ('walking', 'inactive', 'active', 'driving')\n",
    "\n",
    "# Check if X has valid data\n",
    "print(\"Shape of X:\", X.shape)  # Check the shape of the feature matrix\n",
    "\n",
    "# Handle missing data in X\n",
    "X = X.dropna()  # Dropping rows with missing values in features\n",
    "\n",
    "# If X is still empty, that means no data passed the dropna step, which means something went wrong\n",
    "if X.shape[0] == 0:\n",
    "    raise ValueError(\"No valid rows left in the feature set after handling missing data.\")\n",
    "\n",
    "# Step 2: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Scale the features (Random Forest is not very sensitive to feature scaling, but it's still a good practice)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train the Random Forest model with 1000 trees (estimators)\n",
    "rf_model = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d98c9340-d04f-408d-b6a1-ab8df2b4ca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (60733, 54)\n",
      "Model: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1200, n_jobs=None,\n",
      "              num_parallel_tree=None, objective='multi:softprob', ...) Accuracy: 0.9999176751461266\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Active       1.00      1.00      1.00      4419\n",
      "     Driving       1.00      1.00      1.00      4327\n",
      "    Inactive       1.00      1.00      1.00      1046\n",
      "     Walking       1.00      1.00      1.00      2355\n",
      "\n",
      "    accuracy                           1.00     12147\n",
      "   macro avg       1.00      1.00      1.00     12147\n",
      "weighted avg       1.00      1.00      1.00     12147\n",
      "\n",
      "\n",
      "Model: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "              min_child_weight=3, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1200, n_jobs=None,\n",
      "              num_parallel_tree=None, objective='multi:softprob', ...) Accuracy: 0.9999176751461266\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Active       1.00      1.00      1.00      4419\n",
      "     Driving       1.00      1.00      1.00      4327\n",
      "    Inactive       1.00      1.00      1.00      1046\n",
      "     Walking       1.00      1.00      1.00      2355\n",
      "\n",
      "    accuracy                           1.00     12147\n",
      "   macro avg       1.00      1.00      1.00     12147\n",
      "weighted avg       1.00      1.00      1.00     12147\n",
      "\n",
      "\n",
      "Model: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1200, n_jobs=None,\n",
      "              num_parallel_tree=None, objective='multi:softprob', ...) Accuracy: 0.9999176751461266\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Active       1.00      1.00      1.00      4419\n",
      "     Driving       1.00      1.00      1.00      4327\n",
      "    Inactive       1.00      1.00      1.00      1046\n",
      "     Walking       1.00      1.00      1.00      2355\n",
      "\n",
      "    accuracy                           1.00     12147\n",
      "   macro avg       1.00      1.00      1.00     12147\n",
      "weighted avg       1.00      1.00      1.00     12147\n",
      "\n",
      "\n",
      "Model: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
      "              min_child_weight=3, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=1200, n_jobs=None,\n",
      "              num_parallel_tree=None, objective='multi:softprob', ...) Accuracy: 0.9999176751461266\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Active       1.00      1.00      1.00      4419\n",
      "     Driving       1.00      1.00      1.00      4327\n",
      "    Inactive       1.00      1.00      1.00      1046\n",
      "     Walking       1.00      1.00      1.00      2355\n",
      "\n",
      "    accuracy                           1.00     12147\n",
      "   macro avg       1.00      1.00      1.00     12147\n",
      "weighted avg       1.00      1.00      1.00     12147\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming df1 is already loaded and cleaned\n",
    "\n",
    "# Step 1: Prepare the feature matrix (X) and target vector (y)\n",
    "X = df1.drop(columns=['id', 'activity_id', 'user', 'timestamp', 'activity'])  # Features\n",
    "y = df1['activity']  # Target variable ('Walking', 'Inactive', 'Active', 'Driving')\n",
    "\n",
    "# Check if X has valid data\n",
    "print(\"Shape of X:\", X.shape)  # Check the shape of the feature matrix\n",
    "\n",
    "# Handle missing data in X\n",
    "X = X.dropna()  # Dropping rows with missing values in features\n",
    "\n",
    "# If X is still empty, that means no data passed the dropna step, which means something went wrong\n",
    "if X.shape[0] == 0:\n",
    "    raise ValueError(\"No valid rows left in the feature set after handling missing data.\")\n",
    "\n",
    "# Step 2: Label encode the target variable (y) into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Step 3: Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Scale the features (XGBoost is generally not sensitive to feature scaling, but it's good practice)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Train the XGBoost model with specified hyperparameters\n",
    "# Try max_depth=5 and max_depth=8, and min_child_weight=1 and min_child_weight=3\n",
    "xgb_model_5_1 = XGBClassifier(n_estimators=1200, max_depth=5, min_child_weight=1, random_state=42)\n",
    "xgb_model_5_1.fit(X_train_scaled, y_train)\n",
    "\n",
    "xgb_model_5_3 = XGBClassifier(n_estimators=1200, max_depth=5, min_child_weight=3, random_state=42)\n",
    "xgb_model_5_3.fit(X_train_scaled, y_train)\n",
    "\n",
    "xgb_model_8_1 = XGBClassifier(n_estimators=1200, max_depth=8, min_child_weight=1, random_state=42)\n",
    "xgb_model_8_1.fit(X_train_scaled, y_train)\n",
    "\n",
    "xgb_model_8_3 = XGBClassifier(n_estimators=1200, max_depth=8, min_child_weight=3, random_state=42)\n",
    "xgb_model_8_3.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 6: Evaluate each model\n",
    "models = [xgb_model_5_1, xgb_model_5_3, xgb_model_8_1, xgb_model_8_3]\n",
    "for model in models:\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "    print(f\"Model: {model} Accuracy: {accuracy}\\nClassification Report:\\n{report}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc78549-1a07-456c-b875-18e71c99407f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
